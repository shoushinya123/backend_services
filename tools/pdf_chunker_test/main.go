package main

import (
	"fmt"
	"os"
	"strings"
	"time"

	"github.com/aihub/backend-go/internal/knowledge"
)

func main() {
	if len(os.Args) < 2 {
		fmt.Println("Usage: go run main.go <pdf_file> [chunkSize] [overlap]")
		fmt.Println("Example: go run main.go 31092-æ­£æ–‡-1-122.pdf 800 120")
		os.Exit(1)
	}

	pdfPath := os.Args[1]
	chunkSize := 800
	overlap := 120

	if len(os.Args) > 2 {
		fmt.Sscanf(os.Args[2], "%d", &chunkSize)
	}
	if len(os.Args) > 3 {
		fmt.Sscanf(os.Args[3], "%d", &overlap)
	}

	fmt.Println("=" + strings.Repeat("=", 100))
	fmt.Println("PDFæ–‡ä»¶æ™ºèƒ½åˆ†å—æµ‹è¯•")
	fmt.Println("=" + strings.Repeat("=", 100))
	fmt.Printf("PDFæ–‡ä»¶: %s\n", pdfPath)
	fmt.Printf("åˆ†å—é…ç½®: chunkSize=%d, overlap=%d\n", chunkSize, overlap)
	fmt.Printf("æµ‹è¯•æ—¶é—´: %s\n\n", time.Now().Format("2006-01-02 15:04:05"))

	// æ‰“å¼€PDFæ–‡ä»¶
	file, err := os.Open(pdfPath)
	if err != nil {
		fmt.Printf("âŒ é”™è¯¯: æ— æ³•æ‰“å¼€PDFæ–‡ä»¶: %v\n", err)
		os.Exit(1)
	}
	defer file.Close()

	// ä½¿ç”¨PDFParseræå–æ–‡æœ¬
	parser := &knowledge.PDFParser{}
	if !parser.Supports(pdfPath) {
		fmt.Printf("âŒ é”™è¯¯: æ–‡ä»¶ %s ä¸æ˜¯PDFæ ¼å¼\n", pdfPath)
		os.Exit(1)
	}

	fmt.Println("ğŸ“„ æ­£åœ¨æå–PDFæ–‡æœ¬...")
	startParse := time.Now()
	text, err := parser.Parse(file, pdfPath)
	if err != nil {
		fmt.Printf("âŒ é”™è¯¯: PDFè§£æå¤±è´¥: %v\n", err)
		os.Exit(1)
	}
	parseDuration := time.Since(startParse)

	// åˆ†ææ–‡æœ¬
	textRunes := []rune(text)
	textLen := len(textRunes)
	paragraphs := strings.Split(text, "\n\n")
	nonEmptyParagraphs := 0
	for _, p := range paragraphs {
		if strings.TrimSpace(p) != "" {
			nonEmptyParagraphs++
		}
	}

	fmt.Printf("âœ… PDFè§£æå®Œæˆ (è€—æ—¶: %v)\n", parseDuration)
	fmt.Printf("   - æ–‡æœ¬é•¿åº¦: %d å­—ç¬¦\n", textLen)
	fmt.Printf("   - æ®µè½æ•°é‡: %d (éç©º: %d)\n", len(paragraphs), nonEmptyParagraphs)
	fmt.Printf("   - é¢„ä¼°Tokenæ•°: %d (ä¼°ç®—)\n\n", estimateTokens(text))

	// æ‰§è¡Œåˆ†å—
	fmt.Println("ğŸ”ª æ­£åœ¨æ‰§è¡Œæ™ºèƒ½åˆ†å—...")
	startChunk := time.Now()
	chunker := knowledge.NewChunker(chunkSize, overlap)
	chunks := chunker.Split(text)
	chunkDuration := time.Since(startChunk)

	fmt.Printf("âœ… åˆ†å—å®Œæˆ (è€—æ—¶: %v)\n", chunkDuration)
	fmt.Printf("   - åˆ†å—æ•°é‡: %d\n", len(chunks))
	fmt.Printf("   - å¹³å‡æ¯å—: %.1f å­—ç¬¦\n", float64(textLen)/float64(len(chunks)))
	fmt.Println()

	// æ˜¾ç¤ºåˆ†å—ç»“æœ
	fmt.Println("=" + strings.Repeat("=", 100))
	fmt.Println("åˆ†å—ç»“æœè¯¦æƒ…")
	fmt.Println("=" + strings.Repeat("=", 100))

	totalChars := 0
	totalTokens := 0
	semanticBreaks := 0
	nonSemanticBreaks := 0

	for i, chunk := range chunks {
		chars := len([]rune(chunk.Text))
		totalChars += chars
		totalTokens += chunk.TokenCount

		fmt.Println(strings.Repeat("-", 100))
		fmt.Printf("å— #%d (ç´¢å¼•: %d)\n", i+1, chunk.Index)
		fmt.Printf("  å­—ç¬¦æ•°: %d\n", chars)
		fmt.Printf("  Tokenæ•°: %d\n", chunk.TokenCount)
		fmt.Printf("  å¤§å°å æ¯”: %.1f%% (ç›¸å¯¹äºchunkSize=%d)\n", float64(chars)/float64(chunkSize)*100, chunkSize)

		// æ£€æŸ¥è¯­ä¹‰è¾¹ç•Œ
		if i < len(chunks)-1 {
			chunkRunes := []rune(chunk.Text)
			nextChunkRunes := []rune(chunks[i+1].Text)
			if len(chunkRunes) > 0 && len(nextChunkRunes) > 0 {
				lastRune := chunkRunes[len(chunkRunes)-1]
				nextFirstRune := nextChunkRunes[0]
				isSemanticBreak := isSentenceEnd(lastRune) || isParagraphBreak(chunk.Text, chunks[i+1].Text)
				if isSemanticBreak {
					semanticBreaks++
					fmt.Printf("  è¾¹ç•Œ: âœ… è¯­ä¹‰è¾¹ç•Œ\n")
				} else {
					nonSemanticBreaks++
					fmt.Printf("  è¾¹ç•Œ: âš ï¸  éè¯­ä¹‰è¾¹ç•Œ (å—#%dç»“å°¾: '%c', å—#%då¼€å¤´: '%c')\n", 
						i+1, lastRune, i+2, nextFirstRune)
				}
			}
		}

		// æ˜¾ç¤ºå†…å®¹é¢„è§ˆï¼ˆå‰100å­—ç¬¦ï¼‰
		preview := chunk.Text
		if len([]rune(preview)) > 100 {
			preview = string([]rune(preview)[:100]) + "..."
		}
		fmt.Printf("  å†…å®¹é¢„è§ˆ: %s\n", strings.ReplaceAll(preview, "\n", "\\n"))
	}

	// ç»Ÿè®¡ä¿¡æ¯
	fmt.Println("\n" + strings.Repeat("=", 100))
	fmt.Println("åˆ†å—ç»Ÿè®¡")
	fmt.Println(strings.Repeat("=", 100))
	fmt.Printf("æ€»å­—ç¬¦æ•°: %d (åŸå§‹: %d, å·®å¼‚: %d)\n", totalChars, textLen, totalChars-textLen)
	fmt.Printf("æ€»Tokenæ•°: %d (ä¼°ç®—)\n", totalTokens)
	fmt.Printf("å¹³å‡å—å¤§å°: %.1f å­—ç¬¦\n", float64(totalChars)/float64(len(chunks)))
	fmt.Printf("å¹³å‡Tokenæ•°: %.1f\n", float64(totalTokens)/float64(len(chunks)))
	
	if semanticBreaks+nonSemanticBreaks > 0 {
		semanticRate := float64(semanticBreaks) / float64(semanticBreaks+nonSemanticBreaks) * 100
		fmt.Printf("è¯­ä¹‰è¾¹ç•Œä¿æŒç‡: %.1f%% (%d/%d)\n", semanticRate, semanticBreaks, semanticBreaks+nonSemanticBreaks)
	}

	// æ€§èƒ½ç»Ÿè®¡
	fmt.Println("\n" + strings.Repeat("=", 100))
	fmt.Println("æ€§èƒ½ç»Ÿè®¡")
	fmt.Println(strings.Repeat("=", 100))
	fmt.Printf("PDFè§£ææ—¶é—´: %v\n", parseDuration)
	fmt.Printf("åˆ†å—å¤„ç†æ—¶é—´: %v\n", chunkDuration)
	fmt.Printf("æ€»å¤„ç†æ—¶é—´: %v\n", parseDuration+chunkDuration)
	fmt.Printf("å¤„ç†é€Ÿåº¦: %.0f å­—ç¬¦/ç§’\n", float64(textLen)/(parseDuration+chunkDuration).Seconds())

	fmt.Println("\n" + strings.Repeat("=", 100))
}

func isSentenceEnd(r rune) bool {
	return r == 'ã€‚' || r == 'ï¼' || r == 'ï¼Ÿ' || r == '.' || r == '!' || r == '?'
}

func isParagraphBreak(chunk1, chunk2 string) bool {
	chunk1Runes := []rune(chunk1)
	chunk2Runes := []rune(chunk2)
	if len(chunk1Runes) == 0 || len(chunk2Runes) == 0 {
		return false
	}
	return isSentenceEnd(chunk1Runes[len(chunk1Runes)-1]) && 
		   (chunk2Runes[0] >= 'A' && chunk2Runes[0] <= 'Z' || 
		    chunk2Runes[0] >= 'a' && chunk2Runes[0] <= 'z' ||
		    chunk2Runes[0] >= 0x4e00 && chunk2Runes[0] <= 0x9fff)
}

func estimateTokens(text string) int {
	chineseChars := 0
	englishWords := 0
	
	for _, r := range text {
		if r >= 0x4e00 && r <= 0x9fff {
			chineseChars++
		}
	}
	
	words := strings.Fields(text)
	for _, word := range words {
		hasEnglish := false
		for _, r := range word {
			if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') {
				hasEnglish = true
				break
			}
		}
		if hasEnglish {
			englishWords++
		}
	}
	
	estimated := int(float64(chineseChars)*1.5 + float64(englishWords)*1.3)
	if estimated < len(text)/4 {
		estimated = len(text) / 4
	}
	return estimated
}

