package main

import (
	"fmt"
	"os"
	"os/exec"
	"strings"
	"time"

	"github.com/aihub/backend-go/internal/knowledge"
)

func main() {
	if len(os.Args) < 2 {
		fmt.Println("Usage: go run main_v2.go <pdf_file> [chunkSize] [overlap]")
		fmt.Println("Example: go run main_v2.go 31092-æ­£æ–‡-1-122.pdf 800 120")
		os.Exit(1)
	}

	pdfPath := os.Args[1]
	chunkSize := 800
	overlap := 120

	if len(os.Args) > 2 {
		fmt.Sscanf(os.Args[2], "%d", &chunkSize)
	}
	if len(os.Args) > 3 {
		fmt.Sscanf(os.Args[3], "%d", &overlap)
	}

	fmt.Println("=" + strings.Repeat("=", 100))
	fmt.Println("PDFæ–‡ä»¶æ™ºèƒ½åˆ†å—æµ‹è¯•")
	fmt.Println("=" + strings.Repeat("=", 100))
	fmt.Printf("PDFæ–‡ä»¶: %s\n", pdfPath)
	fmt.Printf("åˆ†å—é…ç½®: chunkSize=%d, overlap=%d\n", chunkSize, overlap)
	fmt.Printf("æµ‹è¯•æ—¶é—´: %s\n\n", time.Now().Format("2006-01-02 15:04:05"))

	// ä½¿ç”¨Pythonè„šæœ¬æå–PDFæ–‡æœ¬
	fmt.Println("ðŸ“„ æ­£åœ¨æå–PDFæ–‡æœ¬...")
	startParse := time.Now()
	
	cmd := exec.Command("python3", "tools/pdf_chunker_test/extract_pdf.py", pdfPath)
	output, err := cmd.Output()
	if err != nil {
		fmt.Printf("âŒ é”™è¯¯: PDFè§£æžå¤±è´¥: %v\n", err)
		fmt.Println("æç¤º: è¯·å®‰è£…pdfplumberæˆ–PyPDF2: pip install pdfplumber")
		os.Exit(1)
	}
	
	text := string(output)
	parseDuration := time.Since(startParse)

	// åˆ†æžæ–‡æœ¬
	textRunes := []rune(text)
	textLen := len(textRunes)
	paragraphs := strings.Split(text, "\n\n")
	nonEmptyParagraphs := 0
	for _, p := range paragraphs {
		if strings.TrimSpace(p) != "" {
			nonEmptyParagraphs++
		}
	}

	fmt.Printf("âœ… PDFè§£æžå®Œæˆ (è€—æ—¶: %v)\n", parseDuration)
	fmt.Printf("   - æ–‡æœ¬é•¿åº¦: %d å­—ç¬¦\n", textLen)
	fmt.Printf("   - æ®µè½æ•°é‡: %d (éžç©º: %d)\n", len(paragraphs), nonEmptyParagraphs)
	fmt.Printf("   - é¢„ä¼°Tokenæ•°: %d (ä¼°ç®—)\n\n", estimateTokens(text))

	if textLen == 0 {
		fmt.Println("âš ï¸  è­¦å‘Š: PDFæ–‡æœ¬ä¸ºç©ºï¼Œå¯èƒ½PDFè§£æžå¤±è´¥")
		os.Exit(1)
	}

	// æ‰§è¡Œåˆ†å—
	fmt.Println("ðŸ”ª æ­£åœ¨æ‰§è¡Œæ™ºèƒ½åˆ†å—...")
	startChunk := time.Now()
	chunker := knowledge.NewChunker(chunkSize, overlap)
	chunks := chunker.Split(text)
	chunkDuration := time.Since(startChunk)

	fmt.Printf("âœ… åˆ†å—å®Œæˆ (è€—æ—¶: %v)\n", chunkDuration)
	fmt.Printf("   - åˆ†å—æ•°é‡: %d\n", len(chunks))
	if len(chunks) > 0 {
		fmt.Printf("   - å¹³å‡æ¯å—: %.1f å­—ç¬¦\n", float64(textLen)/float64(len(chunks)))
	}
	fmt.Println()

	// æ˜¾ç¤ºåˆ†å—ç»“æžœï¼ˆé™åˆ¶æ˜¾ç¤ºå‰20ä¸ªå—ï¼‰
	maxDisplay := 20
	if len(chunks) > maxDisplay {
		fmt.Printf("âš ï¸  åˆ†å—æ•°é‡è¾ƒå¤š(%d)ï¼Œä»…æ˜¾ç¤ºå‰%dä¸ªå—\n\n", len(chunks), maxDisplay)
	}

	fmt.Println("=" + strings.Repeat("=", 100))
	fmt.Println("åˆ†å—ç»“æžœè¯¦æƒ…")
	fmt.Println("=" + strings.Repeat("=", 100))

	totalChars := 0
	totalTokens := 0
	semanticBreaks := 0
	nonSemanticBreaks := 0

	displayCount := len(chunks)
	if displayCount > maxDisplay {
		displayCount = maxDisplay
	}

	for i := 0; i < displayCount; i++ {
		chunk := chunks[i]
		chars := len([]rune(chunk.Text))
		totalChars += chars
		totalTokens += chunk.TokenCount

		fmt.Println(strings.Repeat("-", 100))
		fmt.Printf("å— #%d (ç´¢å¼•: %d)\n", i+1, chunk.Index)
		fmt.Printf("  å­—ç¬¦æ•°: %d\n", chars)
		fmt.Printf("  Tokenæ•°: %d\n", chunk.TokenCount)
		fmt.Printf("  å¤§å°å æ¯”: %.1f%% (ç›¸å¯¹äºŽchunkSize=%d)\n", float64(chars)/float64(chunkSize)*100, chunkSize)

		// æ£€æŸ¥è¯­ä¹‰è¾¹ç•Œ
		if i < len(chunks)-1 {
			chunkRunes := []rune(chunk.Text)
			nextChunkRunes := []rune(chunks[i+1].Text)
			if len(chunkRunes) > 0 && len(nextChunkRunes) > 0 {
				lastRune := chunkRunes[len(chunkRunes)-1]
				nextFirstRune := nextChunkRunes[0]
				isSemanticBreak := isSentenceEnd(lastRune) || isParagraphBreak(chunk.Text, chunks[i+1].Text)
				if isSemanticBreak {
					semanticBreaks++
					fmt.Printf("  è¾¹ç•Œ: âœ… è¯­ä¹‰è¾¹ç•Œ\n")
				} else {
					nonSemanticBreaks++
					fmt.Printf("  è¾¹ç•Œ: âš ï¸  éžè¯­ä¹‰è¾¹ç•Œ (å—#%dç»“å°¾: '%c', å—#%då¼€å¤´: '%c')\n", 
						i+1, lastRune, i+2, nextFirstRune)
				}
			}
		}

		// æ˜¾ç¤ºå†…å®¹é¢„è§ˆï¼ˆå‰150å­—ç¬¦ï¼‰
		preview := chunk.Text
		previewRunes := []rune(preview)
		if len(previewRunes) > 150 {
			preview = string(previewRunes[:150]) + "..."
		}
		// æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿æ˜¾ç¤º
		preview = strings.ReplaceAll(preview, "\n", "\\n")
		fmt.Printf("  å†…å®¹é¢„è§ˆ: %s\n", preview)
	}

	// è®¡ç®—æ‰€æœ‰å—çš„ç»Ÿè®¡
	allTotalChars := 0
	allTotalTokens := 0
	for _, chunk := range chunks {
		allTotalChars += len([]rune(chunk.Text))
		allTotalTokens += chunk.TokenCount
	}

	// ç»Ÿè®¡ä¿¡æ¯
	fmt.Println("\n" + strings.Repeat("=", 100))
	fmt.Println("åˆ†å—ç»Ÿè®¡")
	fmt.Println(strings.Repeat("=", 100))
	fmt.Printf("æ€»å—æ•°: %d\n", len(chunks))
	fmt.Printf("æ€»å­—ç¬¦æ•°: %d (åŽŸå§‹: %d, å·®å¼‚: %d)\n", allTotalChars, textLen, allTotalChars-textLen)
	fmt.Printf("æ€»Tokenæ•°: %d (ä¼°ç®—)\n", allTotalTokens)
	fmt.Printf("å¹³å‡å—å¤§å°: %.1f å­—ç¬¦\n", float64(allTotalChars)/float64(len(chunks)))
	fmt.Printf("å¹³å‡Tokenæ•°: %.1f\n", float64(allTotalTokens)/float64(len(chunks)))
	
	if semanticBreaks+nonSemanticBreaks > 0 {
		semanticRate := float64(semanticBreaks) / float64(semanticBreaks+nonSemanticBreaks) * 100
		fmt.Printf("è¯­ä¹‰è¾¹ç•Œä¿æŒçŽ‡: %.1f%% (%d/%d)\n", semanticRate, semanticBreaks, semanticBreaks+nonSemanticBreaks)
	}

	// æ€§èƒ½ç»Ÿè®¡
	fmt.Println("\n" + strings.Repeat("=", 100))
	fmt.Println("æ€§èƒ½ç»Ÿè®¡")
	fmt.Println(strings.Repeat("=", 100))
	fmt.Printf("PDFè§£æžæ—¶é—´: %v\n", parseDuration)
	fmt.Printf("åˆ†å—å¤„ç†æ—¶é—´: %v\n", chunkDuration)
	fmt.Printf("æ€»å¤„ç†æ—¶é—´: %v\n", parseDuration+chunkDuration)
	if (parseDuration + chunkDuration).Seconds() > 0 {
		fmt.Printf("å¤„ç†é€Ÿåº¦: %.0f å­—ç¬¦/ç§’\n", float64(textLen)/(parseDuration+chunkDuration).Seconds())
	}

	fmt.Println("\n" + strings.Repeat("=", 100))
}

func isSentenceEnd(r rune) bool {
	return r == 'ã€‚' || r == 'ï¼' || r == 'ï¼Ÿ' || r == '.' || r == '!' || r == '?'
}

func isParagraphBreak(chunk1, chunk2 string) bool {
	chunk1Runes := []rune(chunk1)
	chunk2Runes := []rune(chunk2)
	if len(chunk1Runes) == 0 || len(chunk2Runes) == 0 {
		return false
	}
	return isSentenceEnd(chunk1Runes[len(chunk1Runes)-1]) && 
		   (chunk2Runes[0] >= 'A' && chunk2Runes[0] <= 'Z' || 
		    chunk2Runes[0] >= 'a' && chunk2Runes[0] <= 'z' ||
		    chunk2Runes[0] >= 0x4e00 && chunk2Runes[0] <= 0x9fff)
}

func estimateTokens(text string) int {
	chineseChars := 0
	englishWords := 0
	
	for _, r := range text {
		if r >= 0x4e00 && r <= 0x9fff {
			chineseChars++
		}
	}
	
	words := strings.Fields(text)
	for _, word := range words {
		hasEnglish := false
		for _, r := range word {
			if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') {
				hasEnglish = true
				break
			}
		}
		if hasEnglish {
			englishWords++
		}
	}
	
	estimated := int(float64(chineseChars)*1.5 + float64(englishWords)*1.3)
	if estimated < len(text)/4 {
		estimated = len(text) / 4
	}
	return estimated
}

